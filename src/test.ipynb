{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('py38jupyter': conda)",
      "metadata": {
        "interpreter": {
          "hash": "ab9111e2d1eb7638a6ee3590029ebb66ef2184c5b4604a96c97d54fc911db079"
        }
      }
    },
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cTu7X1Ujagx"
      },
      "source": [
        "taken from [link](https://keras.io/examples/generative/lstm_character_level_text_generation/)\n",
        "  \n",
        "data from [link](https://www.kaggle.com/namanj27/astronomers-telegram-dataset?select=Processed_Atels.csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcF6qaOtjag1"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e314meGxjag3"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlHLtqH-jag4"
      },
      "source": [
        "# Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-SlUdHlkWiz"
      },
      "source": [
        "data = pd.read_csv(\"Processed_Atels.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87zx22Uyjag5",
        "outputId": "e80d10fc-a1e8-4d7a-dd7d-a3ba48ab8eb2"
      },
      "source": [
        "text_processed = data[\"Text processed\"]\n",
        "text = \"\"\n",
        "for row in text_processed:\n",
        "    text += row\n",
        "\n",
        "print(\"Corpus length:\", len(text))\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# print(char_indices)\n",
        "# print(indices_char)\n",
        "\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 1940271\n",
            "Total chars: 108\n",
            "Number of sequences: 646744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AKv5bN8MrIEK",
        "outputId": "691ea689-5937-4005-8f22-90ac9bbe9cb5"
      },
      "source": [
        "text[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We report '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo8Rr0jZjag5"
      },
      "source": [
        "# Build the model: a single LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY6swYiwjag6"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(maxlen, len(chars))),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ-M4y0bjag6"
      },
      "source": [
        "# Prepare the text sampling function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNMsMuQAjag6"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUsG58Djag7"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-qFEt3qjag7",
        "outputId": "62c71375-2dfa-4351-b322-23846f1ca2fc"
      },
      "source": [
        "epochs = 1\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
        "    print()\n",
        "    print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    print(f\"text len {len(text)} start_index {start_index}, maxlen {maxlen}\")\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print(\"...Diversity:\", diversity)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index : start_index + maxlen]\n",
        "        print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "        # for i in range(400):\n",
        "        #     x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        #     for t, char in enumerate(sentence):\n",
        "        #         x_pred[0, t, char_indices[char]] = 1.0\n",
        "        #     preds = model.predict(x_pred, verbose=0)[0]\n",
        "        #     next_index = sample(preds, diversity)\n",
        "        #     next_char = indices_char[next_index]\n",
        "        #     sentence = sentence[1:] + next_char\n",
        "        #     generated += next_char\n",
        "\n",
        "        print(\"...Generated: \", generated)\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5053/5053 [==============================] - 27s 5ms/step - loss: 1.3312\n",
            "\n",
            "Generating text after epoch: 0\n",
            "text len 1940271 start_index 1511779, maxlen 40\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"V. It is the product of an international\"\n",
            "...Generated:  \n",
            "\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"V. It is the product of an international\"\n",
            "...Generated:  \n",
            "\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"V. It is the product of an international\"\n",
            "...Generated:  \n",
            "\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \"V. It is the product of an international\"\n",
            "...Generated:  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q-B2aU5Djag7",
        "outputId": "7b89f77e-ba96-4fd9-cf0b-53087d4e0a92"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We report spectroscopic observations of AT2018lab, discovered by the DLT40 survey on UT 2018 Dec 29.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}